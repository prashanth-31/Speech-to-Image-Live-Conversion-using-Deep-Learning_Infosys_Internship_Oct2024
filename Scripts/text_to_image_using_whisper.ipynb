{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf6392c-8b6f-43fd-89ff-a9a71e0c7c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import queue\n",
    "import time\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a42274-eae7-4541-b56f-ff559a02ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prash\\anaconda3\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base\") \n",
    "\n",
    "# Initialize audio queue\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Callback function for audio streaming\n",
    "def callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(status)\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "# Function to transcribe audio\n",
    "def transcribe_audio(duration):\n",
    "    start_time = time.time()\n",
    "    audio_data_list = []\n",
    "\n",
    "    # Collect audio data for the specified duration\n",
    "    while time.time() - start_time < duration:\n",
    "        audio_data = audio_queue.get()\n",
    "        audio_data_list.append(audio_data)\n",
    "\n",
    "    # Process audio data\n",
    "    audio_data_combined = np.concatenate(audio_data_list, axis=0)\n",
    "    audio_data_combined = np.squeeze(audio_data_combined)\n",
    "\n",
    "    # Prepare audio for Whisper\n",
    "    audio_data_combined = whisper.pad_or_trim(audio_data_combined)\n",
    "    mel = whisper.log_mel_spectrogram(audio_data_combined).to(model.device)\n",
    "\n",
    "    # Decode audio to text\n",
    "    options = whisper.DecodingOptions(language=\"en\")\n",
    "    result = model.decode(mel, options)\n",
    "    print(\"Transcription:\", result.text)\n",
    "    return result.text\n",
    "\n",
    "# Function to generate an image using Stable Diffusion\n",
    "def generate_image(prompt):\n",
    "    # Load Stable Diffusion model\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(\"cuda\")\n",
    "    pipe.scheduler.num_inference_steps = 50\n",
    "\n",
    "    # Configure parameters\n",
    "    guidance_scale = 8.0  \n",
    "    print(f\"Generating image for prompt: '{prompt}'\")\n",
    "\n",
    "    # Generate image\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        generated_image = pipe(prompt, guidance_scale=guidance_scale).images[0]\n",
    "\n",
    "    # Display the generated image\n",
    "    generated_image.show()\n",
    "    display(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5687676-4fe8-472d-a3d0-4367b61de0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for 15 seconds...\n",
      "Transcription: The futuristic cityscape was sunset with flying cars.\n",
      "Finished recording.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0f8c0546164101b92308752584e759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image for prompt: 'The futuristic cityscape was sunset with flying cars.'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec242036aaa5485caab3a74931cddb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    duration = 15  \n",
    "\n",
    "    # Start recording and transcribing audio\n",
    "    with sd.InputStream(callback=callback, channels=1, samplerate=16000):\n",
    "        print(\"Recording for 15 seconds...\")\n",
    "        transcription = transcribe_audio(duration)\n",
    "        print(\"Finished recording.\")\n",
    "\n",
    "    # Use transcription as a prompt for image generation\n",
    "    generate_image(transcription)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3aa0c-ccc6-4e49-9a48-b27925425cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
